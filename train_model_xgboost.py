# -*- coding: utf-8 -*-
"""JPMorgan_Task_3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KVu1pEydROi7dKC1kqih62bkrxhBlGZ-
"""

# Data and preprocessing
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample

# Evaluation
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score

# Model
import xgboost as xgb

# Saving
import pickle
import os

# Load the dataset
raw_data = pd.read_csv('Task_3_and_4_Loan_Data.csv', index_col=0)

# BALANCE DATASET
# Separate majority and minority classes
target_column = 'default'
df_majority = raw_data[raw_data[target_column] == raw_data[target_column].value_counts().idxmax()]
df_minority = raw_data[raw_data[target_column] == raw_data[target_column].value_counts().idxmin()]

# Downsample majority class
df_majority_downsampled = resample(df_majority, replace=False, n_samples=len(df_minority),random_state=42)

# Combine minority and downsampled majority class
data_balanced = pd.concat([df_minority, df_majority_downsampled])

# Shuffle dataset
data_balanced = data_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

data_balanced[target_column].value_counts()

data_unbalanced = raw_data.copy()

data_cleaned = data_unbalanced.copy()
data_cleaned = data_cleaned.apply(lambda x: x.replace("?",np.nan).astype('float'))

numerical_features = data_cleaned.columns
categorical_features = []
###
for col in numerical_features:
  data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].median())
for col in categorical_features:
  data_cleaned[col] = data_cleaned[col].fillna(data_cleaned[col].mode()[0])


y = data_cleaned['default']
X = data_cleaned.drop('default', axis=1)

## split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr_best = 0.01
n_estimators_best = 500
max_depth_best = 7

xg_model = xgb.XGBClassifier(learning_rate=lr_best, n_estimators=n_estimators_best, max_depth=max_depth_best, random_state=42)
xg_model.fit(X_train, y_train)

y_pred = xg_model.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Precision:", precision_score(y_test, y_pred))
print("Recall:", recall_score(y_test, y_pred))
print("F1 Score:", f1_score(y_test, y_pred))


# Create models directory and save model
os.makedirs('models', exist_ok=True)

with open('models/xgboost_model.pkl', 'wb') as f:
    pickle.dump(xg_model, f)

print("Model trained and saved successfully!")

with open("models/feature_names.txt", "w") as f:
    f.write("\n".join(X.columns))

